{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0b65d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "DATAPATH = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5a6277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "npa = pd.read_csv(os.path.join(DATAPATH, \"original\", \"active_NP.csv\"))\n",
    "npi = pd.read_csv(os.path.join(DATAPATH, \"original\", \"inactive_NP.csv\"))\n",
    "sda = pd.read_csv(os.path.join(DATAPATH, \"original\", \"active_SD.csv\"))\n",
    "sdi = pd.read_csv(os.path.join(DATAPATH, \"original\", \"inactive_SD.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33f38b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(356, 20) (39, 21) (423, 19) (50, 17)\n",
      "(356, 20) (39, 21) (422, 19) (50, 17)\n"
     ]
    }
   ],
   "source": [
    "print(npa.shape, npi.shape, sda.shape, sdi.shape)\n",
    "npa = npa[~npa[\"smiles\"].isna()]\n",
    "npi = npi[~npi[\"smiles\"].isna()]\n",
    "sda = sda[~sda[\"smiles\"].isna()]\n",
    "sdi = sdi[~sdi[\"smiles\"].isna()]\n",
    "print(npa.shape, npi.shape, sda.shape, sdi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2ac5298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356 39 422 50\n"
     ]
    }
   ],
   "source": [
    "smi_npa = npa[\"smiles\"]\n",
    "smi_npi = npi[\"smiles\"]\n",
    "smi_sda = sda[\"smiles\"]\n",
    "smi_sdi = sdi[\"smiles\"]\n",
    "print(len(smi_npa), len(smi_npi), len(smi_sda), len(smi_sdi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f24a5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:22:23] SMILES Parse Error: syntax error while parsing: 1[C@H](C([C@](C(C1(C(=O)O)O)(C(=O)C2=CC(=C(C(=C2)O)O)O)C(=O)C3=CC(=C(C(=C3)O)O)O)(C(=O)C4=CC(=C(C(=C4)O)O)O)O)(C(=O)C5=CC(=C(C(=C5)O)O)O)O)O\n",
      "[11:22:23] SMILES Parse Error: check for mistakes around position 1:\n",
      "[11:22:23] 1[C@H](C([C@](C(C1(C(=O)O)O)(C(=O)C2=CC(=\n",
      "[11:22:23] ^\n",
      "[11:22:23] SMILES Parse Error: Failed parsing SMILES '1[C@H](C([C@](C(C1(C(=O)O)O)(C(=O)C2=CC(=C(C(=C2)O)O)O)C(=O)C3=CC(=C(C(=C3)O)O)O)(C(=O)C4=CC(=C(C(=C4)O)O)O)O)(C(=O)C5=CC(=C(C(=C5)O)O)O)O)O' for input: '1[C@H](C([C@](C(C1(C(=O)O)O)(C(=O)C2=CC(=C(C(=C2)O)O)O)C(=O)C3=CC(=C(C(=C3)O)O)O)(C(=O)C4=CC(=C(C(=C4)O)O)O)O)(C(=O)C5=CC(=C(C(=C5)O)O)O)O)O'\n",
      "[11:22:23] SMILES Parse Error: syntax error while parsing: CC(N=C[23])=NC(C(F)(F)F)=C@23CCC(N(@23)CC[6]=CN=C(C[13]()=CC=CC=C@13C[22]=NN=NN@22)C=C@7)=O\n",
      "[11:22:23] SMILES Parse Error: check for mistakes around position 10:\n",
      "[11:22:23] CC(N=C[23])=NC(C(F)(F)F)=C@23CCC(N(@23)CC\n",
      "[11:22:23] ~~~~~~~~~^\n",
      "[11:22:23] SMILES Parse Error: Failed parsing SMILES 'CC(N=C[23])=NC(C(F)(F)F)=C@23CCC(N(@23)CC[6]=CN=C(C[13]()=CC=CC=C@13C[22]=NN=NN@22)C=C@7)=O' for input: 'CC(N=C[23])=NC(C(F)(F)F)=C@23CCC(N(@23)CC[6]=CN=C(C[13]()=CC=CC=C@13C[22]=NN=NN@22)C=C@7)=O'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before standardization: (356, 21) (39, 22) (422, 20) (50, 18)\n",
      "After standardization: (355, 21) (39, 22) (418, 20) (50, 18)\n",
      "Internal duplicates in npa: 10 unique duplicate SMILES\n",
      "Internal duplicates in npi: 2 unique duplicate SMILES\n",
      "Internal duplicates in sda: 0 unique duplicate SMILES\n",
      "Internal duplicates in sdi: 0 unique duplicate SMILES\n",
      "Duplicates Active/Inactive NP: 6\n",
      "Duplicates Active/Inactive SD: 0\n",
      "Duplicates NP/SD: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:22:23] SMILES Parse Error: syntax error while parsing: O=C(O)CN(C(C(C)CSC(C)=O)=O)C[6:S=N]HC[S=I](C[12])(H)CCC[S=I]@12(H)C@7\n",
      "[11:22:23] SMILES Parse Error: check for mistakes around position 31:\n",
      "[11:22:23] (C(C)CSC(C)=O)=O)C[6:S=N]HC[S=I](C[12])(H\n",
      "[11:22:23] ~~~~~~~~~~~~~~~~~~~~^\n",
      "[11:22:23] SMILES Parse Error: Failed parsing SMILES 'O=C(O)CN(C(C(C)CSC(C)=O)=O)C[6:S=N]HC[S=I](C[12])(H)CCC[S=I]@12(H)C@7' for input: 'O=C(O)CN(C(C(C)CSC(C)=O)=O)C[6:S=N]HC[S=I](C[12])(H)CCC[S=I]@12(H)C@7'\n",
      "[11:22:23] SMILES Parse Error: syntax error while parsing: O=C(O)CN(CCC[17]=CC=CO@17)C(C(C)CSC(C)=O)=O\n",
      "[11:22:23] SMILES Parse Error: check for mistakes around position 16:\n",
      "[11:22:23] O=C(O)CN(CCC[17]=CC=CO@17)C(C(C)CSC(C)=O)\n",
      "[11:22:23] ~~~~~~~~~~~~~~~^\n",
      "[11:22:23] SMILES Parse Error: Failed parsing SMILES 'O=C(O)CN(CCC[17]=CC=CO@17)C(C(C)CSC(C)=O)=O' for input: 'O=C(O)CN(CCC[17]=CC=CO@17)C(C(C)CSC(C)=O)=O'\n",
      "[11:22:23] SMILES Parse Error: syntax error while parsing: O=C(O)CN(C[17]=C(C)C=CC=C@17)C(C(C)CS)=C\n",
      "[11:22:23] SMILES Parse Error: check for mistakes around position 14:\n",
      "[11:22:23] O=C(O)CN(C[17]=C(C)C=CC=C@17)C(C(C)CS)=C\n",
      "[11:22:23] ~~~~~~~~~~~~~^\n",
      "[11:22:23] SMILES Parse Error: Failed parsing SMILES 'O=C(O)CN(C[17]=C(C)C=CC=C@17)C(C(C)CS)=C' for input: 'O=C(O)CN(C[17]=C(C)C=CC=C@17)C(C(C)CS)=C'\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "def parse_and_standardize(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None, None\n",
    "        canonical_smiles = Chem.MolToSmiles(mol, canonical=True)\n",
    "        inchikey = Chem.inchi.MolToInchiKey(mol)\n",
    "        return canonical_smiles, inchikey\n",
    "    except:\n",
    "        return None, None\n",
    "npa[['canonical_smiles', 'inchikey']] = npa['smiles'].apply(lambda x: pd.Series(parse_and_standardize(x)))\n",
    "npi[['canonical_smiles', 'inchikey']] = npi['smiles'].apply(lambda x: pd.Series(parse_and_standardize(x)))\n",
    "sda[['canonical_smiles', 'inchikey']] = sda['smiles'].apply(lambda x: pd.Series(parse_and_standardize(x)))\n",
    "sdi[['canonical_smiles', 'inchikey']] = sdi['smiles'].apply(lambda x: pd.Series(parse_and_standardize(x)))\n",
    "\n",
    "# Remove rows without canonical SMILES\n",
    "print(\"Before standardization:\", npa.shape, npi.shape, sda.shape, sdi.shape)\n",
    "npa_clean = npa.dropna(subset=['canonical_smiles']).copy()\n",
    "npi_clean = npi.dropna(subset=['canonical_smiles']).copy()\n",
    "sda_clean = sda.dropna(subset=['canonical_smiles']).copy()\n",
    "sdi_clean = sdi.dropna(subset=['canonical_smiles']).copy()\n",
    "print(\"After standardization:\", npa_clean.shape, npi_clean.shape, sda_clean.shape, sdi_clean.shape)\n",
    "\n",
    "# Duplicates\n",
    "duplicates_in_npa = npa_clean[npa_clean.duplicated(subset=['canonical_smiles'], keep=False)].copy()\n",
    "duplicates_in_npi = npi_clean[npi_clean.duplicated(subset=['canonical_smiles'], keep=False)].copy()\n",
    "duplicates_in_sda = sdi_clean[sdi_clean.duplicated(subset=['canonical_smiles'], keep=False)].copy()\n",
    "duplicates_in_sdi = sdi_clean[sdi_clean.duplicated(subset=['canonical_smiles'], keep=False)].copy()\n",
    "print(f\"Internal duplicates in npa: {duplicates_in_npa['canonical_smiles'].nunique()} unique duplicate SMILES\")\n",
    "print(f\"Internal duplicates in npi: {duplicates_in_npi['canonical_smiles'].nunique()} unique duplicate SMILES\")\n",
    "print(f\"Internal duplicates in sda: {duplicates_in_sda['canonical_smiles'].nunique()} unique duplicate SMILES\")\n",
    "print(f\"Internal duplicates in sdi: {duplicates_in_sdi['canonical_smiles'].nunique()} unique duplicate SMILES\")\n",
    "duplicates_in_npa.to_csv(os.path.join(DATAPATH, \"processed\", 'duplicates_within_npa.csv'), index=False)\n",
    "duplicates_in_npi.to_csv(os.path.join(DATAPATH, \"processed\",'duplicates_within_npi.csv'), index=False)\n",
    "duplicates_in_sda.to_csv(os.path.join(DATAPATH, \"processed\", 'duplicates_within_sda.csv'), index=False)\n",
    "duplicates_in_sdi.to_csv(os.path.join(DATAPATH, \"processed\",'duplicates_within_sdi.csv'), index=False)\n",
    "\n",
    "#keep one occurrence of duplicate molecules within each set\n",
    "npa_final = npa_clean.drop_duplicates(subset=['canonical_smiles'], keep='first') \n",
    "npi_final = npi_clean.drop_duplicates(subset=['canonical_smiles'], keep='first')\n",
    "sda_final = sda_clean.drop_duplicates(subset=['canonical_smiles'], keep='first')\n",
    "sdi_final = sdi_clean.drop_duplicates(subset=['canonical_smiles'], keep='first')\n",
    "\n",
    "# Duplicates between sets\n",
    "set1 = set(npa_final['canonical_smiles'])\n",
    "set2 = set(npi_final['canonical_smiles'])\n",
    "set3 = set(sda_final['canonical_smiles'])\n",
    "set4 = set(sdi_final['canonical_smiles'])\n",
    "# Intersection between NP Active and Inactive\n",
    "common_smiles = set1.intersection(set2)\n",
    "print(\"Duplicates Active/Inactive NP:\", len(common_smiles))\n",
    "common_smiles_npa = npa_final[npa_final['canonical_smiles'].isin(common_smiles)]\n",
    "common_smiles_npi = npi_final[npi_final['canonical_smiles'].isin(common_smiles)]\n",
    "common_smiles_npa.to_csv(os.path.join(DATAPATH, \"processed\", 'duplicates_npa_with_npi.csv'), index=False)\n",
    "common_smiles_npi.to_csv(os.path.join(DATAPATH, \"processed\", 'duplicates_npi_with_npa.csv'), index=False)\n",
    "# Intersection between SD Active and Inactive\n",
    "common_smiles = set3.intersection(set4)\n",
    "print(\"Duplicates Active/Inactive SD:\", len(common_smiles))\n",
    "# Intersection between NP and SD\n",
    "np = set1.union(set2)\n",
    "sd = set3.union(set4)\n",
    "common_smiles = np.intersection(sd)\n",
    "print(\"Duplicates NP/SD:\", len(common_smiles))\n",
    "\n",
    "# Remove the common SMILES between Active and Inactive NP from the Inactive set\n",
    "npi_final = npi_final[~npi_final['canonical_smiles'].isin(common_smiles_npi['canonical_smiles'])]\n",
    "\n",
    "#Save cleaned data\n",
    "npa_final.to_csv(os.path.join(DATAPATH, \"processed\", \"npa_clean.csv\"), index=False)\n",
    "npi_final.to_csv(os.path.join(DATAPATH, \"processed\", \"npi_clean.csv\"), index=False)\n",
    "sda_final.to_csv(os.path.join(DATAPATH, \"processed\", \"sda_clean.csv\"), index=False)\n",
    "sdi_final.to_csv(os.path.join(DATAPATH, \"processed\", \"sdi_clean.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e4fbcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset has 835 molecules.\n",
      "835 unique SMILES in the final dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_724174/1535982522.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  npa_final[\"activity\"] = 1\n",
      "/tmp/ipykernel_724174/1535982522.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sda_final[\"activity\"] = 1\n",
      "/tmp/ipykernel_724174/1535982522.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  npa_final['category'] = 'natural'\n",
      "/tmp/ipykernel_724174/1535982522.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sda_final['category'] = 'synthetic'\n"
     ]
    }
   ],
   "source": [
    "#merge into one file for general analysis\n",
    "npa_final[\"activity\"] = 1\n",
    "npi_final[\"activity\"] = 0\n",
    "sda_final[\"activity\"] = 1\n",
    "sdi_final[\"activity\"] = 0\n",
    "\n",
    "npa_final['category'] = 'natural'\n",
    "npi_final['category'] = 'natural'\n",
    "sda_final['category'] = 'synthetic'\n",
    "sdi_final['category'] = 'synthetic'\n",
    "\n",
    "final_df = pd.concat([\n",
    "    npa_final[['id', 'canonical_smiles', 'inchikey', 'category', 'activity']],\n",
    "    npi_final[['id', 'canonical_smiles', 'inchikey', 'category', 'activity']],\n",
    "    sda_final[['id', 'canonical_smiles', 'inchikey', 'category', 'activity']],\n",
    "    sdi_final[['id', 'canonical_smiles', 'inchikey', 'category', 'activity']]\n",
    "], ignore_index=True)\n",
    "\n",
    "final_df.to_csv(os.path.join(DATAPATH, \"processed\", \"all_molecules.csv\"), index=False)\n",
    "\n",
    "print(f\"Final dataset has {len(final_df)} molecules.\")\n",
    "print(len(set(final_df['canonical_smiles'])), \"unique SMILES in the final dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2dc1a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset for ersilia\n",
    "final_df = pd.read_csv(os.path.join(DATAPATH, \"processed\", \"all_molecules.csv\"))\n",
    "ersilia_df = final_df[['canonical_smiles']]\n",
    "ersilia_df = ersilia_df.to_csv(os.path.join(DATAPATH, \"processed\", \"all_smiles.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710209c3",
   "metadata": {},
   "source": [
    "# Assay Specific Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92455033",
   "metadata": {},
   "outputs": [],
   "source": [
    "npa = pd.read_csv(os.path.join(DATAPATH, \"original\", \"active_NP.csv\"))\n",
    "npi = pd.read_csv(os.path.join(DATAPATH, \"original\", \"inactive_NP.csv\"))\n",
    "sda = pd.read_csv(os.path.join(DATAPATH, \"original\", \"active_SD.csv\"))\n",
    "sdi = pd.read_csv(os.path.join(DATAPATH, \"original\", \"inactive_SD.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c052dba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(868, 27)\n",
      "(867, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13:12:20] SMILES Parse Error: syntax error while parsing: 1[C@H](C([C@](C(C1(C(=O)O)O)(C(=O)C2=CC(=C(C(=C2)O)O)O)C(=O)C3=CC(=C(C(=C3)O)O)O)(C(=O)C4=CC(=C(C(=C4)O)O)O)O)(C(=O)C5=CC(=C(C(=C5)O)O)O)O)O\n",
      "[13:12:20] SMILES Parse Error: check for mistakes around position 1:\n",
      "[13:12:20] 1[C@H](C([C@](C(C1(C(=O)O)O)(C(=O)C2=CC(=\n",
      "[13:12:20] ^\n",
      "[13:12:20] SMILES Parse Error: Failed parsing SMILES '1[C@H](C([C@](C(C1(C(=O)O)O)(C(=O)C2=CC(=C(C(=C2)O)O)O)C(=O)C3=CC(=C(C(=C3)O)O)O)(C(=O)C4=CC(=C(C(=C4)O)O)O)O)(C(=O)C5=CC(=C(C(=C5)O)O)O)O)O' for input: '1[C@H](C([C@](C(C1(C(=O)O)O)(C(=O)C2=CC(=C(C(=C2)O)O)O)C(=O)C3=CC(=C(C(=C3)O)O)O)(C(=O)C4=CC(=C(C(=C4)O)O)O)O)(C(=O)C5=CC(=C(C(=C5)O)O)O)O)O'\n",
      "[13:12:21] SMILES Parse Error: syntax error while parsing: CC(N=C[23])=NC(C(F)(F)F)=C@23CCC(N(@23)CC[6]=CN=C(C[13]()=CC=CC=C@13C[22]=NN=NN@22)C=C@7)=O\n",
      "[13:12:21] SMILES Parse Error: check for mistakes around position 10:\n",
      "[13:12:21] CC(N=C[23])=NC(C(F)(F)F)=C@23CCC(N(@23)CC\n",
      "[13:12:21] ~~~~~~~~~^\n",
      "[13:12:21] SMILES Parse Error: Failed parsing SMILES 'CC(N=C[23])=NC(C(F)(F)F)=C@23CCC(N(@23)CC[6]=CN=C(C[13]()=CC=CC=C@13C[22]=NN=NN@22)C=C@7)=O' for input: 'CC(N=C[23])=NC(C(F)(F)F)=C@23CCC(N(@23)CC[6]=CN=C(C[13]()=CC=CC=C@13C[22]=NN=NN@22)C=C@7)=O'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(862, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13:12:21] SMILES Parse Error: syntax error while parsing: O=C(O)CN(C(C(C)CSC(C)=O)=O)C[6:S=N]HC[S=I](C[12])(H)CCC[S=I]@12(H)C@7\n",
      "[13:12:21] SMILES Parse Error: check for mistakes around position 31:\n",
      "[13:12:21] (C(C)CSC(C)=O)=O)C[6:S=N]HC[S=I](C[12])(H\n",
      "[13:12:21] ~~~~~~~~~~~~~~~~~~~~^\n",
      "[13:12:21] SMILES Parse Error: Failed parsing SMILES 'O=C(O)CN(C(C(C)CSC(C)=O)=O)C[6:S=N]HC[S=I](C[12])(H)CCC[S=I]@12(H)C@7' for input: 'O=C(O)CN(C(C(C)CSC(C)=O)=O)C[6:S=N]HC[S=I](C[12])(H)CCC[S=I]@12(H)C@7'\n",
      "[13:12:21] SMILES Parse Error: syntax error while parsing: O=C(O)CN(CCC[17]=CC=CO@17)C(C(C)CSC(C)=O)=O\n",
      "[13:12:21] SMILES Parse Error: check for mistakes around position 16:\n",
      "[13:12:21] O=C(O)CN(CCC[17]=CC=CO@17)C(C(C)CSC(C)=O)\n",
      "[13:12:21] ~~~~~~~~~~~~~~~^\n",
      "[13:12:21] SMILES Parse Error: Failed parsing SMILES 'O=C(O)CN(CCC[17]=CC=CO@17)C(C(C)CSC(C)=O)=O' for input: 'O=C(O)CN(CCC[17]=CC=CO@17)C(C(C)CSC(C)=O)=O'\n",
      "[13:12:21] SMILES Parse Error: syntax error while parsing: O=C(O)CN(C[17]=C(C)C=CC=C@17)C(C(C)CS)=C\n",
      "[13:12:21] SMILES Parse Error: check for mistakes around position 14:\n",
      "[13:12:21] O=C(O)CN(C[17]=C(C)C=CC=C@17)C(C(C)CS)=C\n",
      "[13:12:21] ~~~~~~~~~~~~~^\n",
      "[13:12:21] SMILES Parse Error: Failed parsing SMILES 'O=C(O)CN(C[17]=C(C)C=CC=C@17)C(C(C)CS)=C' for input: 'O=C(O)CN(C[17]=C(C)C=CC=C@17)C(C(C)CS)=C'\n"
     ]
    }
   ],
   "source": [
    "npa[\"category\"] = \"natural\"\n",
    "npi['category'] = 'natural'\n",
    "sda['category'] = 'synthetic'\n",
    "sdi['category'] = 'synthetic'\n",
    "\n",
    "all = pd.concat([npa, npi, sda, sdi], axis=0)\n",
    "print(all.shape)\n",
    "all = all[~all[\"smiles\"].isna()]\n",
    "print(all.shape)\n",
    "\n",
    "def parse_and_standardize(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None, None\n",
    "        canonical_smiles = Chem.MolToSmiles(mol, canonical=True)\n",
    "        inchikey = Chem.inchi.MolToInchiKey(mol)\n",
    "        return canonical_smiles, inchikey\n",
    "    except:\n",
    "        return None, None\n",
    "all[['canonical_smiles', 'inchikey']] = all['smiles'].apply(lambda x: pd.Series(parse_and_standardize(x)))\n",
    "all = all.dropna(subset=['canonical_smiles']).copy()\n",
    "print(all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d9587a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280 40 12 12 119 51\n"
     ]
    }
   ],
   "source": [
    "all = all[[\"canonical_smiles\", \"inchikey\", \"category\", \"activity\", \"target\"]]\n",
    "all[\"activity\"] = all[\"activity\"].map({\"Active\": 1, \"Inactive\": 0})\n",
    "ace = all[all[\"target\"].str.contains(\"ACE\", case=False, na=False)]\n",
    "enos = all[all[\"target\"].str.contains(\"ENOS\", case=False, na=False)]\n",
    "at = all[all[\"target\"].str.contains(\"AT1\", case=False, na=False)]\n",
    "at1rec = all[all[\"target\"].str.contains(\"AT1 Receptor|AT1R\", case=False, na=False)]\n",
    "ca = all[all[\"target\"].str.contains(\"CA2\", case=False, na=False)]\n",
    "k = all[all[\"target\"].str.contains(\"K+\", case=False, na=False)]\n",
    "print(len(ace), len(enos), len(at), len(at1rec), len(ca), len(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f0551a",
   "metadata": {},
   "source": [
    "# Duplicates will be removed, Actives prioritised over inactives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d3822e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247 33 126 154\n",
      "117 2 80 39\n",
      "50 1 28 23\n"
     ]
    }
   ],
   "source": [
    "print(len(ace[ace[\"activity\"]==1]),len(ace[ace[\"activity\"]==0]),len(ace[ace[\"category\"]==\"natural\"]), len(ace[ace[\"category\"]==\"synthetic\"]))\n",
    "print(len(ca[ca[\"activity\"]==1]),len(ca[ca[\"activity\"]==0]), len(ca[ca[\"category\"]==\"natural\"]), len(ca[ca[\"category\"]==\"synthetic\"]))\n",
    "print(len(k[k[\"activity\"]==1]),len(k[k[\"activity\"]==0]), len(k[k[\"category\"]==\"natural\"]), len(k[k[\"category\"]==\"synthetic\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "94765a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>canonical_smiles</th>\n",
       "      <th>inchikey</th>\n",
       "      <th>category</th>\n",
       "      <th>activity</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>O=c1cc(-c2ccc(O)c(O)c2)oc2cc(O)cc(O)c12</td>\n",
       "      <td>IQPNAANSBPBGFQ-UHFFFAOYSA-N</td>\n",
       "      <td>natural</td>\n",
       "      <td>1</td>\n",
       "      <td>ACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>O=c1c(O[C@@H]2O[C@H](CO)[C@@H](O)[C@H](O)[C@H]...</td>\n",
       "      <td>OVSQVDMCBVZWGM-QSOFNFLRSA-N</td>\n",
       "      <td>natural</td>\n",
       "      <td>1</td>\n",
       "      <td>ACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O=c1cc(-c2ccc(O)c(O)c2)oc2cc(O)cc(O)c12</td>\n",
       "      <td>IQPNAANSBPBGFQ-UHFFFAOYSA-N</td>\n",
       "      <td>natural</td>\n",
       "      <td>0</td>\n",
       "      <td>ACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>O=c1c(O[C@@H]2O[C@H](CO)[C@@H](O)[C@H](O)[C@H]...</td>\n",
       "      <td>OVSQVDMCBVZWGM-QSOFNFLRSA-N</td>\n",
       "      <td>natural</td>\n",
       "      <td>0</td>\n",
       "      <td>ACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>O=c1c(O[C@@H]2O[C@H](CO)[C@@H](O)[C@H](O)[C@H]...</td>\n",
       "      <td>OVSQVDMCBVZWGM-QSOFNFLRSA-N</td>\n",
       "      <td>natural</td>\n",
       "      <td>0</td>\n",
       "      <td>ACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>CCOC(=O)[C@H](CCc1ccccc1)N[C@@H](C)C(=O)N1CCC[...</td>\n",
       "      <td>GBXSMTUPTTWBMN-XIRDDKMYSA-N</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>1</td>\n",
       "      <td>ACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>CCOC(=O)[C@H](CCc1ccccc1)N[C@@H](C)C(=O)N1CCC[...</td>\n",
       "      <td>GBXSMTUPTTWBMN-XIRDDKMYSA-N</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>1</td>\n",
       "      <td>ACE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      canonical_smiles  \\\n",
       "154            O=c1cc(-c2ccc(O)c(O)c2)oc2cc(O)cc(O)c12   \n",
       "167  O=c1c(O[C@@H]2O[C@H](CO)[C@@H](O)[C@H](O)[C@H]...   \n",
       "3              O=c1cc(-c2ccc(O)c(O)c2)oc2cc(O)cc(O)c12   \n",
       "9    O=c1c(O[C@@H]2O[C@H](CO)[C@@H](O)[C@H](O)[C@H]...   \n",
       "10   O=c1c(O[C@@H]2O[C@H](CO)[C@@H](O)[C@H](O)[C@H]...   \n",
       "195  CCOC(=O)[C@H](CCc1ccccc1)N[C@@H](C)C(=O)N1CCC[...   \n",
       "305  CCOC(=O)[C@H](CCc1ccccc1)N[C@@H](C)C(=O)N1CCC[...   \n",
       "\n",
       "                        inchikey   category  activity target  \n",
       "154  IQPNAANSBPBGFQ-UHFFFAOYSA-N    natural         1    ACE  \n",
       "167  OVSQVDMCBVZWGM-QSOFNFLRSA-N    natural         1    ACE  \n",
       "3    IQPNAANSBPBGFQ-UHFFFAOYSA-N    natural         0    ACE  \n",
       "9    OVSQVDMCBVZWGM-QSOFNFLRSA-N    natural         0    ACE  \n",
       "10   OVSQVDMCBVZWGM-QSOFNFLRSA-N    natural         0    ACE  \n",
       "195  GBXSMTUPTTWBMN-XIRDDKMYSA-N  synthetic         1    ACE  \n",
       "305  GBXSMTUPTTWBMN-XIRDDKMYSA-N  synthetic         1    ACE  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ace[ace.duplicated(subset=['canonical_smiles'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1b456672",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only ace of the three has duplicates\n",
    "ace = ace.sort_values(by=\"activity\", ascending=False)\n",
    "ace= ace.drop_duplicates(subset=\"canonical_smiles\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "48bc4834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246 30 123 153\n",
      "117 2 80 39\n",
      "50 1 28 23\n"
     ]
    }
   ],
   "source": [
    "print(len(ace[ace[\"activity\"]==1]),len(ace[ace[\"activity\"]==0]),len(ace[ace[\"category\"]==\"natural\"]), len(ace[ace[\"category\"]==\"synthetic\"]))\n",
    "print(len(ca[ca[\"activity\"]==1]),len(ca[ca[\"activity\"]==0]), len(ca[ca[\"category\"]==\"natural\"]), len(ca[ca[\"category\"]==\"synthetic\"]))\n",
    "print(len(k[k[\"activity\"]==1]),len(k[k[\"activity\"]==0]), len(k[k[\"category\"]==\"natural\"]), len(k[k[\"category\"]==\"synthetic\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5760d7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ace.to_csv(os.path.join(DATAPATH, \"processed\", \"ace.csv\"), index=False)\n",
    "ca.to_csv(os.path.join(DATAPATH, \"processed\", \"ca.csv\"), index=False)\n",
    "k.to_csv(os.path.join(DATAPATH, \"processed\", \"k.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5720614b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "print(len(k[(k[\"category\"]==\"natural\")&(k[\"activity\"]==1)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
