{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b65d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "DATAPATH = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5a6277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(os.path.join(DATAPATH, \"original\", \"250425_sheet1.csv\"))\n",
    "df2 = pd.read_csv(os.path.join(DATAPATH, \"original\", \"250425_sheet2.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33f38b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513, 21) (467, 18)\n",
      "(490, 21) (465, 18)\n"
     ]
    }
   ],
   "source": [
    "print(df1.shape, df2.shape)\n",
    "df1 = df1[~df1[\"Smiles\"].isna()]\n",
    "df2 = df2[~df2[\"Smiles\"].isna()]\n",
    "print(df1.shape, df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60552e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(490, 21) (465, 18)\n",
      "(486, 21) (281, 18)\n"
     ]
    }
   ],
   "source": [
    "#Assume they are all active and this is a mistake\n",
    "print(df1.shape, df2.shape)\n",
    "df1_ = df1[~df1[\"Activity\"].isna()]\n",
    "df2_ = df2[~df2[\"Activity\"].isna()]\n",
    "print(df1_.shape, df2_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2ac5298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490 465\n"
     ]
    }
   ],
   "source": [
    "smi1 = df1[\"Smiles\"]\n",
    "smi2 = df2[\"Smiles\"]\n",
    "print(len(smi1), len(smi2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f24a5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:33:09] SMILES Parse Error: syntax error while parsing: C1=C(C=C(C(=C1O)O)O)C(=O)OC[C@@H]2[C@H]([C@@H]([C@H([C@@H(O2)OC(=O)C3=CC(=C(C(=C3)O)O)O)OC(=O)C4=CC(=C(C(=C4)O)O)O)OC(=O)C5=CC(=C(C(=C5)O)O)O)OC(=O)C6=CC(=C(C(=C6)O)O)O\n",
      "[12:33:09] SMILES Parse Error: Failed parsing SMILES 'C1=C(C=C(C(=C1O)O)O)C(=O)OC[C@@H]2[C@H]([C@@H]([C@H([C@@H(O2)OC(=O)C3=CC(=C(C(=C3)O)O)O)OC(=O)C4=CC(=C(C(=C4)O)O)O)OC(=O)C5=CC(=C(C(=C5)O)O)O)OC(=O)C6=CC(=C(C(=C6)O)O)O' for input: 'C1=C(C=C(C(=C1O)O)O)C(=O)OC[C@@H]2[C@H]([C@@H]([C@H([C@@H(O2)OC(=O)C3=CC(=C(C(=C3)O)O)O)OC(=O)C4=CC(=C(C(=C4)O)O)O)OC(=O)C5=CC(=C(C(=C5)O)O)O)OC(=O)C6=CC(=C(C(=C6)O)O)O'\n",
      "[12:33:09] SMILES Parse Error: syntax error while parsing: CSCPPACGZOOCGX-UHFFFAOYSA-N\n",
      "[12:33:09] SMILES Parse Error: Failed parsing SMILES 'CSCPPACGZOOCGX-UHFFFAOYSA-N' for input: 'CSCPPACGZOOCGX-UHFFFAOYSA-N'\n",
      "[12:33:09] SMILES Parse Error: syntax error while parsing: 1[C@H](C([C@](C(C1(C(=O)O)O)(C(=O)C2=CC(=C(C(=C2)O)O)O)C(=O)C3=CC(=C(C(=C3)O)O)O)(C(=O)C4=CC(=C(C(=C4)O)O)O)O)(C(=O)C5=CC(=C(C(=C5)O)O)O)O)O\n",
      "[12:33:09] SMILES Parse Error: Failed parsing SMILES '1[C@H](C([C@](C(C1(C(=O)O)O)(C(=O)C2=CC(=C(C(=C2)O)O)O)C(=O)C3=CC(=C(C(=C3)O)O)O)(C(=O)C4=CC(=C(C(=C4)O)O)O)O)(C(=O)C5=CC(=C(C(=C5)O)O)O)O)O' for input: '1[C@H](C([C@](C(C1(C(=O)O)O)(C(=O)C2=CC(=C(C(=C2)O)O)O)C(=O)C3=CC(=C(C(=C3)O)O)O)(C(=O)C4=CC(=C(C(=C4)O)O)O)O)(C(=O)C5=CC(=C(C(=C5)O)O)O)O)O'\n",
      "[12:33:09] SMILES Parse Error: syntax error while parsing: CC(N=C[23])=NC(C(F)(F)F)=C@23CCC(N(@23)CC[6]=CN=C(C[13]()=CC=CC=C@13C[22]=NN=NN@22)C=C@7)=O\n",
      "[12:33:09] SMILES Parse Error: Failed parsing SMILES 'CC(N=C[23])=NC(C(F)(F)F)=C@23CCC(N(@23)CC[6]=CN=C(C[13]()=CC=CC=C@13C[22]=NN=NN@22)C=C@7)=O' for input: 'CC(N=C[23])=NC(C(F)(F)F)=C@23CCC(N(@23)CC[6]=CN=C(C[13]()=CC=CC=C@13C[22]=NN=NN@22)C=C@7)=O'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before standardization: (490, 23) (465, 20)\n",
      "After standardization: (487, 23) (459, 20)\n",
      "Internal duplicates in df1: 63 unique duplicate SMILES\n",
      "Internal duplicates in df2: 27 unique duplicate SMILES\n",
      "Number of duplicates between df1 and df2: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:33:09] SMILES Parse Error: syntax error while parsing: O=C(C[3:S=I]HN(CCC@4)C(C[S=N]H(NC[S=N]H(CCC[16]=CC=CC=C@16)C(O)=O)C)=O)O\n",
      "[12:33:09] SMILES Parse Error: Failed parsing SMILES 'O=C(C[3:S=I]HN(CCC@4)C(C[S=N]H(NC[S=N]H(CCC[16]=CC=CC=C@16)C(O)=O)C)=O)O' for input: 'O=C(C[3:S=I]HN(CCC@4)C(C[S=N]H(NC[S=N]H(CCC[16]=CC=CC=C@16)C(O)=O)C)=O)O'\n",
      "[12:33:09] SMILES Parse Error: syntax error while parsing: O=C(O)CN(C(C(C)CSC(C)=O)=O)C[6:S=N]HC[S=I](C[12])(H)CCC[S=I]@12(H)C@7\n",
      "[12:33:09] SMILES Parse Error: Failed parsing SMILES 'O=C(O)CN(C(C(C)CSC(C)=O)=O)C[6:S=N]HC[S=I](C[12])(H)CCC[S=I]@12(H)C@7' for input: 'O=C(O)CN(C(C(C)CSC(C)=O)=O)C[6:S=N]HC[S=I](C[12])(H)CCC[S=I]@12(H)C@7'\n",
      "[12:33:09] SMILES Parse Error: syntax error while parsing: O=C(O)CN(CCC[17]=CC=CO@17)C(C(C)CSC(C)=O)=O\n",
      "[12:33:09] SMILES Parse Error: Failed parsing SMILES 'O=C(O)CN(CCC[17]=CC=CO@17)C(C(C)CSC(C)=O)=O' for input: 'O=C(O)CN(CCC[17]=CC=CO@17)C(C(C)CSC(C)=O)=O'\n",
      "[12:33:09] SMILES Parse Error: syntax error while parsing: O=C(O)CN(C[17]=C(C)C=CC=C@17)C(C(C)CS)=C\n",
      "[12:33:09] SMILES Parse Error: Failed parsing SMILES 'O=C(O)CN(C[17]=C(C)C=CC=C@17)C(C(C)CS)=C' for input: 'O=C(O)CN(C[17]=C(C)C=CC=C@17)C(C(C)CS)=C'\n",
      "[12:33:09] SMILES Parse Error: syntax error while parsing: O=C(N[18]CCCC)C[16]=C(N=CC(F)=C@16)N(@18)CC(C=C[12])=CC=C@12C[5]=C(C[23]=NNN=N@23)C=CC=C@6\n",
      "[12:33:09] SMILES Parse Error: Failed parsing SMILES 'O=C(N[18]CCCC)C[16]=C(N=CC(F)=C@16)N(@18)CC(C=C[12])=CC=C@12C[5]=C(C[23]=NNN=N@23)C=CC=C@6' for input: 'O=C(N[18]CCCC)C[16]=C(N=CC(F)=C@16)N(@18)CC(C=C[12])=CC=C@12C[5]=C(C[23]=NNN=N@23)C=CC=C@6'\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "def parse_and_standardize(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None, None\n",
    "        canonical_smiles = Chem.MolToSmiles(mol, canonical=True)\n",
    "        inchikey = Chem.inchi.MolToInchiKey(mol)\n",
    "        return canonical_smiles, inchikey\n",
    "    except:\n",
    "        return None, None\n",
    "df1[['canonical_smiles', 'inchikey']] = df1['Smiles'].apply(lambda x: pd.Series(parse_and_standardize(x)))\n",
    "df2[['canonical_smiles', 'inchikey']] = df2['Smiles'].apply(lambda x: pd.Series(parse_and_standardize(x)))\n",
    "\n",
    "# Remove rows without canonical SMILES\n",
    "print(\"Before standardization:\", df1.shape, df2.shape)\n",
    "df1_clean = df1.dropna(subset=['canonical_smiles']).copy()\n",
    "df2_clean = df2.dropna(subset=['canonical_smiles']).copy()\n",
    "print(\"After standardization:\", df1_clean.shape, df2_clean.shape)\n",
    "\n",
    "# Duplicates\n",
    "duplicates_in_df1 = df1_clean[df1_clean.duplicated(subset=['canonical_smiles'], keep=False)].copy()\n",
    "duplicates_in_df2 = df2_clean[df2_clean.duplicated(subset=['canonical_smiles'], keep=False)].copy()\n",
    "print(f\"Internal duplicates in df1: {duplicates_in_df1['canonical_smiles'].nunique()} unique duplicate SMILES\")\n",
    "print(f\"Internal duplicates in df2: {duplicates_in_df2['canonical_smiles'].nunique()} unique duplicate SMILES\")\n",
    "duplicates_in_df1.to_csv(os.path.join(DATAPATH, \"processed\", 'duplicates_within_df1.csv'), index=False)\n",
    "duplicates_in_df2.to_csv(os.path.join(DATAPATH, \"processed\",'duplicates_within_df2.csv'), index=False)\n",
    "\n",
    "df1_final = df1_clean.drop_duplicates(subset=['canonical_smiles'])\n",
    "df2_final = df2_clean.drop_duplicates(subset=['canonical_smiles'])\n",
    "\n",
    "#Save cleaned data\n",
    "df1_final.to_csv(os.path.join(DATAPATH, \"processed\", \"df1_clean.csv\"), index=False)\n",
    "df2_final.to_csv(os.path.join(DATAPATH, \"processed\", \"df2_clean.csv\"), index=False)\n",
    "\n",
    "# Duplicates between sets\n",
    "set1 = set(df1_final['canonical_smiles'])\n",
    "set2 = set(df2_final['canonical_smiles'])\n",
    "common_smiles = set1.intersection(set2)\n",
    "common_smiles_df1 = df1_final[df1_final['canonical_smiles'].isin(common_smiles)]\n",
    "common_smiles_df2 = df2_final[df2_final['canonical_smiles'].isin(common_smiles)]\n",
    "common_smiles_df1.to_csv(os.path.join(DATAPATH, \"processed\", 'duplicates_df1_with_df2.csv'), index=False)\n",
    "common_smiles_df2.to_csv(os.path.join(DATAPATH, \"processed\", 'duplicates_df2_with_df1.csv'), index=False)\n",
    "print(f\"Number of duplicates between df1 and df2: {len(common_smiles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e4fbcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset has 813 molecules.\n"
     ]
    }
   ],
   "source": [
    "#merge into one file\n",
    "\n",
    "df1_final['category'] = 'natural'\n",
    "df2_final['category'] = 'synthetic'\n",
    "\n",
    "df1_final = df1_final[~df1_final['canonical_smiles'].isin(common_smiles)]\n",
    "df2_final = df2_final[~df2_final['canonical_smiles'].isin(common_smiles)]\n",
    "\n",
    "final_df = pd.concat([\n",
    "    df1_final[['id', 'canonical_smiles', 'inchikey', 'category']],\n",
    "    df2_final[['id', 'canonical_smiles', 'inchikey', 'category']]\n",
    "], ignore_index=True)\n",
    "\n",
    "final_df.to_csv(os.path.join(DATAPATH, \"processed\", \"all_molecules.csv\"), index=False)\n",
    "\n",
    "print(f\"Final dataset has {len(final_df)} molecules.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2dc1a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset for ersilia\n",
    "final_df = pd.read_csv(os.path.join(DATAPATH, \"processed\", \"all_molecules.csv\"))\n",
    "ersilia_df = final_df[['canonical_smiles', 'id']]\n",
    "ersilia_df = ersilia_df.to_csv(os.path.join(DATAPATH, \"processed\", \"all_smiles.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
